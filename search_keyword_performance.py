#!/usr/bin/env python3
"""
search_keyword_performance.py
--------------------------------------

This script reads a tab‑delimited hit‑level dataset exported from Adobe Analytics
and computes the total revenue generated by visitors who arrive via search
engines.  For each combination of search engine domain and search keyword, the
script sums the revenue and writes a tab‑delimited report sorted by revenue
descending.

Usage::

    python search_keyword_performance.py path/to/input_file.tsv

The output file is written into the current working directory and named
``YYYY‑mm‑dd_SearchKeywordPerformance.tab`` where the date corresponds to the
execution date of the script.

Key assumptions:
* Search engines considered: Google (`google.com`), Bing (`bing.com`), Yahoo
  (`yahoo.com`), and MSN (`msn.com`).  The domain match is case‑insensitive and
  uses a simple “endswith” test so `www.google.co.uk` would still be treated
  as Google.
* Search keywords are extracted from common query parameters (`q` for
  Google/Bing/MSN, `p` for Yahoo).  If no standard parameter is present, the
  first query parameter is used as a fallback.
* A purchase is recognised when the `event_list` column contains the event
  code ``1`` as per the Adobe Analytics documentation【379017961565072†L144-L156】.
* Revenue for each product is read from the fourth field of each product entry
  in the `product_list` column【379017961565072†L167-L179】.  Products within the
  `product_list` are separated by commas and their attributes are separated by
  semicolons.  If the revenue field is missing or empty, it is ignored.

"""

import csv
import sys
import datetime
from urllib.parse import urlparse, parse_qs


SEARCH_ENGINES = {
    "google.com": "q",  # query parameter used for search term
    "bing.com": "q",
    "yahoo.com": "p",
    "msn.com": "q",
}


def normalize_domain(domain: str) -> str:
    """Return the second‑level domain for matching search engines.

    For example, ``www.google.co.uk`` becomes ``google.co.uk`` and
    ``search.yahoo.com`` becomes ``yahoo.com``.
    """
    if not domain:
        return ""
    parts = domain.lower().split(".")
    # Keep the last two parts for domains like google.com or yahoo.co.uk
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return domain.lower()


def extract_keyword(referrer: str, param_name: str) -> str:
    """Extract the search keyword from the referrer URL using the given parameter name.

    If the specified parameter is not present, fall back to the first query
    parameter found.
    """
    try:
        parsed = urlparse(referrer)
    except Exception:
        return ""
    query_params = parse_qs(parsed.query)
    # Try specified param
    if param_name in query_params and query_params[param_name]:
        # Join multiple values with space if present
        return " ".join(query_params[param_name][0].split("+"))
    # Fallback: use first query parameter
    for values in query_params.values():
        if values:
            return " ".join(values[0].split("+"))
    return ""


def parse_product_list(product_list_str: str) -> float:
    """Parse the product_list column and sum revenue for all products.

    Each product is separated by commas.  Within each product, fields are
    separated by semicolons.  The fourth field (index 3) contains the total
    revenue【379017961565072†L167-L179】.  If the field is empty or cannot be
    converted to a float, it is ignored.
    """
    if not product_list_str:
        return 0.0
    total_revenue = 0.0
    # Split by comma to separate products; handle nested comma inside quotes
    products = []
    current = []
    inside_quote = False
    for char in product_list_str:
        if char == '"':
            inside_quote = not inside_quote
            current.append(char)
            continue
        if char == ',' and not inside_quote:
            products.append(''.join(current))
            current = []
        else:
            current.append(char)
    if current:
        products.append(''.join(current))
    for product in products:
        fields = product.split(';')
        if len(fields) >= 4:
            revenue_str = fields[3]
            if revenue_str:
                try:
                    total_revenue += float(revenue_str)
                except ValueError:
                    pass
    return total_revenue


def process_file(input_path: str) -> dict:
    """Read the input TSV file and return aggregated revenue by (engine, keyword)."""
    results = {}
    with open(input_path, 'r', newline='', encoding='utf-8', errors='ignore') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            referrer = row.get('referrer', '').strip()
            if not referrer:
                continue
            parsed = urlparse(referrer)
            domain = normalize_domain(parsed.hostname or '')
            # Find matching search engine and param name
            matched_engine = None
            param_name = None
            for se_domain, qparam in SEARCH_ENGINES.items():
                if domain.endswith(se_domain):
                    matched_engine = se_domain
                    param_name = qparam
                    break
            if not matched_engine:
                continue  # not a search engine we care about
            # Check event list for purchase
            event_list = row.get('event_list', '').strip()
            if not event_list:
                continue
            events = [e.strip() for e in event_list.split(',') if e.strip()]
            if '1' not in events:
                continue  # only count purchases【379017961565072†L144-L156】
            # Extract keyword
            keyword = extract_keyword(referrer, param_name)
            if not keyword:
                keyword = '(unknown)'
            # Sum revenue
            product_list = row.get('product_list', '').strip()
            revenue = parse_product_list(product_list)
            if revenue <= 0:
                continue
            key = (matched_engine, keyword)
            results[key] = results.get(key, 0.0) + revenue
    return results


def write_output(results: dict) -> str:
    """Write the aggregated results to a tab‑delimited file sorted by revenue.

    Returns the path to the output file.
    """
    date_str = datetime.date.today().strftime('%Y-%m-%d')
    output_filename = f"{date_str}_SearchKeywordPerformance.tab"
    with open(output_filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow(['Search Engine Domain', 'Search Keyword', 'Revenue'])
        # Sort by revenue descending
        for (engine, keyword), revenue in sorted(results.items(), key=lambda x: x[1], reverse=True):
            writer.writerow([engine, keyword, f"{revenue:.2f}"])
    return output_filename


def main():
    if len(sys.argv) != 2:
        print("Usage: python search_keyword_performance.py path/to/input_file.tsv")
        sys.exit(1)
    input_path = sys.argv[1]
    results = process_file(input_path)
    output_path = write_output(results)
    print(f"Results written to {output_path}")


if __name__ == '__main__':
    main()